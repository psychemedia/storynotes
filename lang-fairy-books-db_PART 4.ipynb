{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f800e6bb",
   "metadata": {},
   "source": [
    "## Common Refrains / Repeating Phrases\n",
    "\n",
    "Many stories incorporate a repeating phrase or refrain in the story, but you may need to read quite a long way into a story before you can identify that repeating phrase. So are there any tools we might be able to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "37d0d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The House In The Wood\n"
     ]
    }
   ],
   "source": [
    "#db = Database(db_name)\n",
    "              \n",
    "q2 = '\"pretty hen\"'\n",
    "\n",
    "_q = f'SELECT * FROM books_fts WHERE books_fts MATCH {db.quote(q2)} ;'\n",
    "\n",
    "for row in db.query(_q):\n",
    "    print(row[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d11e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "\n",
    "tokens = nltk.word_tokenize(row[\"text\"])\n",
    "\n",
    "size = 5\n",
    "#for i in nltk_ngrams(tokens, size):\n",
    "#    print(' '.join(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78461b3",
   "metadata": {},
   "source": [
    "We could then look for repeating phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42d7abf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ", pretty brindled cow ,        4\n",
       "And you , pretty brindled      4\n",
       "you , pretty brindled cow      4\n",
       "pretty brindled cow , What     4\n",
       "brindled cow , What do         4\n",
       "                              ..\n",
       "leaving him all day without    1\n",
       "for leaving him all day        1\n",
       "wife for leaving him all       1\n",
       "his wife for leaving him       1\n",
       "to go hungry . '               1\n",
       "Name: phrase, Length: 1787, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'phrase':[' '.join(i) for i in nltk_ngrams(tokens, size)]})\n",
    "df['phrase'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cae8cc",
   "metadata": {},
   "source": [
    "Really, we need to do a scan down from large token size until we find a match (longest match phrase).\n",
    "\n",
    "But for now, let's see what repeating elements we get from one of those search phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4baf60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "\"pretty hen\":  1566 1585 \n",
      "The man said:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "'Duks,' answered the beast\n",
      "===\n",
      "\"pretty hen\":  3505 3524 ed the beasts:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "The beasts answered, 'Duks\n",
      "===\n",
      "\"pretty hen\":  4932 4951  beasts again:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "'Duks,' they said. Then th\n",
      "===\n",
      "\"pretty hen\":  6119 6138  to rest now?'\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "The animals said, 'Duks:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "_q = 'pretty brindled cow'\n",
    "\n",
    "for m in re.finditer(_q, row[\"text\"]):\n",
    "    # Display the matched terms and the 50 characters\n",
    "    # immediately preceding and following the phrase \n",
    "    print(f'===\\n{q2}: ', m.start(), m.end(), row[\"text\"][max(0, m.start()-50):m.end()+50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101aea1",
   "metadata": {},
   "source": [
    "Make a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53ab3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The man said:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "'Duks,' answered the beast \n",
      "==\n",
      "ed the beasts:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "The beasts answered, 'Duks \n",
      "==\n",
      " beasts again:\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "'Duks,' they said. Then th \n",
      "==\n",
      " to rest now?'\n",
      "\n",
      "Pretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\n",
      "\n",
      "The animals said, 'Duks:\n",
      "\n",
      " \n",
      "==\n"
     ]
    }
   ],
   "source": [
    "def find_contexts(text, phrase, width=50):\n",
    "    \"\"\"Find the context(s) of the phrase.\"\"\"\n",
    "    contexts = []\n",
    "    for m in re.finditer(phrase, text):\n",
    "        # Display the matched terms and the `width` characters\n",
    "        # immediately preceding and following the phrase \n",
    "        contexts.append(text[max(0, m.start()-width):m.end()+width])\n",
    "    return contexts\n",
    "\n",
    "for i in find_contexts(row['text'], 'pretty brindled cow'):\n",
    "    print(i,\"\\n==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2b2eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nThe man said:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' answered the beast\",\n",
       " \"ed the beasts:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe beasts answered, 'Duks\",\n",
       " \" beasts again:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' they said. Then th\",\n",
       " \" to rest now?'\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe animals said, 'Duks:\\n\\n\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_contexts(row['text'], 'pretty brindled cow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0db0b",
   "metadata": {},
   "source": [
    "We can also make this a SQLite lookup function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45ec5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtfunc import TableFunction\n",
    "\n",
    "def concordances(text, phrase, width=50):\n",
    "    \"\"\"Find the concordances of a phrase in a text.\"\"\"\n",
    "    contexts = []\n",
    "    for m in re.finditer(phrase, text):\n",
    "        # Display the matched terms and the `width` characters\n",
    "        # immediately preceding and following the phrase\n",
    "        context = text[max(0, m.start()-width):m.end()+width]\n",
    "        contexts.append( (context, m.start(), m.end()) )\n",
    "    return contexts\n",
    "\n",
    "\n",
    "class Concordances(TableFunction):\n",
    "    params = ['phrase', 'text']\n",
    "    columns = ['match', 'start', 'end']\n",
    "    name = 'concordance'\n",
    "\n",
    "    def initialize(self, phrase=None, text=None):\n",
    "        self._iter = iter(concordances(text, phrase))\n",
    "\n",
    "    def iterate(self, idx):\n",
    "        (context, start, end) = next(self._iter)\n",
    "        return (context, start, end,)\n",
    "\n",
    "Concordances.register(db.conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42b68b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"\\nThe man said:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' answered the beast\",\n",
       "  1566,\n",
       "  1585),\n",
       " (\"ed the beasts:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe beasts answered, 'Duks\",\n",
       "  3505,\n",
       "  3524),\n",
       " (\" beasts again:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' they said. Then th\",\n",
       "  4932,\n",
       "  4951),\n",
       " (\" to rest now?'\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe animals said, 'Duks:\\n\\n\",\n",
       "  6119,\n",
       "  6138)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concordances(row['text'], 'pretty brindled cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2b4e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"\\nThe man said:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' answered the beast\", 1566, 1585)\n",
      "(\"ed the beasts:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe beasts answered, 'Duks\", 3505, 3524)\n",
      "(\" beasts again:\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\n'Duks,' they said. Then th\", 4932, 4951)\n",
      "(\" to rest now?'\\n\\nPretty cock, Pretty hen, And you, pretty brindled cow, What do you say now?\\n\\nThe animals said, 'Duks:\\n\\n\", 6119, 6138)\n"
     ]
    }
   ],
   "source": [
    "for i in db.execute('SELECT matched.* FROM books, concordance(\"pretty brindled cow\", books.text) as matched WHERE title=\"The House In The Wood\";'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b076e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow different tokenisers\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def scanner(text, minlen=4, startlen=50, min_repeats = 3, autostop=True):\n",
    "    \"\"\"Search a text for repeated phrases above a minimum length.\"\"\"\n",
    "    # Tokenise the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    #nltk_ngrams returns an empty list if we ask for an ngram longer than the sentence\n",
    "    # So set the (long) start length to the lesser of the original provided\n",
    "    # start length or the token length of the original text;\n",
    "    # which is to say, the minimum of the provided start length \n",
    "    # or the length of the text\n",
    "    startlen = min(startlen, len(tokens))\n",
    "    \n",
    "    # Start with a long sequence then iterate down to a minumum length sequence\n",
    "    for size in range(startlen, minlen-1, -1):\n",
    "        # Generate a dataframe containing all the ngrams, one row per ngram\n",
    "        df = pd.DataFrame({'phrase':[' '.join(i) for i in nltk_ngrams(tokens, size)]})\n",
    "        \n",
    "        # Find the occurrence counts of each phrase\n",
    "        value_counts_series = df['phrase'].value_counts()\n",
    "\n",
    "        # If we have at least the specified number of occurrences\n",
    "        # don't bother searching for any more\n",
    "        if max(value_counts_series) >= min_repeats:\n",
    "            if autostop:\n",
    "                break\n",
    "            pass\n",
    "    # Return a pandas series (an indexed list, essentially)\n",
    "    # containing the longest (or phrases) we found\n",
    "    \n",
    "    return value_counts_series[(value_counts_series>=min_repeats) & (value_counts_series==max(value_counts_series))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d375d001",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-0f91c30a6c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscanner\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-e2f02733262b>\u001b[0m in \u001b[0;36mscanner\u001b[0;34m(text, minlen, startlen, min_repeats, autostop)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Search a text for repeated phrases above a minimum length.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Tokenise the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eighty-seven miles to go, yet.  Onward!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "scanner( row[\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first (0'th indexed) item\n",
    "# (In this case there is only one item hat repeats this number of times anyway.)\n",
    "scanner( row[\"text\"] ).index[0], scanner( row[\"text\"] ).values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7885eef",
   "metadata": {},
   "source": [
    "If we constrain this function to return a single item, we can create a simple SQLite function that will search through records and return the longest phrase above a certain minimum length (or the first longest phrase, if several long phrases of the same length are found):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeating_phrase(text):\n",
    "    \"\"\"Return the longest repeating phrase found in a text.\n",
    "       If there are more than one of the same length, return the first.\n",
    "    \"\"\"\n",
    "    phrase = scanner(text)\n",
    "    \n",
    "    #If there is at least one response, take the first\n",
    "    if not phrase.empty:\n",
    "        return phrase.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_repeating_phrase(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `db` object is a sqlite_utils database object\n",
    "# Pass in:\n",
    "# - the name of the function we want to use in the database\n",
    "# - the number of arguments it takes\n",
    "# - the function we want to invoke\n",
    "db.conn.create_function('find_repeating_phrase', 1, find_repeating_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a132b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_q = \"\"\"\n",
    "SELECT book, title, find_repeating_phrase(text) AS phrase \n",
    "FROM books WHERE title=\"The House In The Wood\" ;\n",
    "\"\"\"\n",
    "\n",
    "for row2 in db.query(_q):\n",
    "    print(row2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_q = \"\"\"\n",
    "SELECT title, find_repeating_phrase(text) AS phrase\n",
    "FROM books WHERE book=\"The Pink Fairy Book\" ;\n",
    "\"\"\"\n",
    "\n",
    "for row3 in db.query(_q):\n",
    "    if row3['phrase'] is not None:\n",
    "        print(row3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d0e20",
   "metadata": {},
   "source": [
    "The punctuation gets in the way somewhat, so it might be useful if removed the punctuation and tried again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow param and de-punctuate\n",
    "\n",
    "def scanner2(text, minlen=4, startlen=50, min_repeats = 4, autostop=True, tokeniser='word'):\n",
    "    \"\"\"Search a text for repeated phrases above a minimum length.\"\"\"\n",
    "    # Tokenise the text\n",
    "    if tokeniser == 'depunc_word':\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "    elif tokeniser == 'sent':\n",
    "        pass\n",
    "    else:\n",
    "        # eg for default: tokeniser='word'\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    #nltk_ngrams returns an empty list if we ask for an ngram longer than the sentence\n",
    "    # So set the (long) start length to the lesser of the original provided\n",
    "    # start length or the token length of the original text;\n",
    "    # which is to say, the minimum of the provided start length \n",
    "    # or the lenth of the text\n",
    "    startlen = min(startlen, len(tokens))\n",
    "    \n",
    "    # Start with a long sequence then iterate down to a minumum length sequence\n",
    "    for size in range(startlen, minlen-1, -1):\n",
    "        \n",
    "        # Generate a dataframe containing all the ngrams, one row per ngram\n",
    "        df = pd.DataFrame({'phrase':[' '.join(i) for i in nltk_ngrams(tokens,size)]})\n",
    "        \n",
    "        # Find the occurrence counts of each phrase\n",
    "        value_counts_series = df['phrase'].value_counts()\n",
    "        \n",
    "        # If we have at least the specified number of occurrences\n",
    "        # don't bother searching for any more\n",
    "        if max(value_counts_series) >= min_repeats:\n",
    "            if autostop:\n",
    "                break\n",
    "            pass\n",
    "    # Return a pandas series (an indexed list, essentially)\n",
    "    # containing the long phrase (or phrases) we found\n",
    "    return value_counts_series[(value_counts_series>=min_repeats) & (value_counts_series==max(value_counts_series))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea926bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeating_phrase_depunc(text, minlen):\n",
    "    \"\"\"Return the longest repeating phrase found in a text.\n",
    "       If there are more than one of the same lentgh, return the first.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accepts a specified minimum phrase length (minlin)\n",
    "    # Reduce the required number of repeats\n",
    "    phrase = scanner2(text, minlen=minlen, min_repeats = 3, tokeniser='depunc_word')\n",
    "    \n",
    "    #If there is at least one response, take the first\n",
    "    if not phrase.empty:\n",
    "        return phrase.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_repeating_phrase_depunc(row['text'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bad69",
   "metadata": {},
   "source": [
    "Register the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we need to update the number of arguments (max. 2)\n",
    "db.conn.create_function('find_repeating_phrase_depunc', 2, find_repeating_phrase_depunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904275d",
   "metadata": {},
   "source": [
    "Try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf28447",
   "metadata": {},
   "outputs": [],
   "source": [
    "_q = \"\"\"\n",
    "SELECT book, title, find_repeating_phrase_depunc(text, 7) AS phrase\n",
    "FROM books WHERE book=\"The Pink Fairy Book\" ;\n",
    "\"\"\"\n",
    "\n",
    "for row5 in db.query(_q):\n",
    "    if row5['phrase'] is not None:\n",
    "        print(row5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b134339",
   "metadata": {},
   "source": [
    "Check the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346baf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "_q = \"\"\"\n",
    "SELECT text, find_repeating_phrase(text) AS phrase\n",
    "FROM books WHERE title=\"Maiden Bright-Eye\" ;\n",
    "\"\"\"\n",
    "\n",
    "for row6 in db.query(_q):\n",
    "    for c in find_contexts(row6['text'], \"Where is my wicked \", 100):\n",
    "        print(c,\"\\n===\")\n",
    "    #print(row6['phrase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac39935",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row6 in db.query(_q):\n",
    "    for c in find_contexts(row6['text'], \"the king's palace\", 100):\n",
    "        print(c,\"\\n===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1e259",
   "metadata": {},
   "source": [
    "We need to be able to find short sentences down to the minimum that are not in a longer phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanner_all(text, minlen=4, startlen=50, min_repeats = 4, autostop=True):\n",
    "    long_phrases = {}\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for size in range(startlen, minlen-1, -1):\n",
    "        df = pd.DataFrame({'phrase':[' '.join(i) for i in nltk_ngrams(tokens, min(size, len(tokens)))]})\n",
    "        value_counts_series = df['phrase'].value_counts()\n",
    "        \n",
    "        if max(value_counts_series) >= min_repeats:\n",
    "            test_phrases = value_counts_series[value_counts_series==max(value_counts_series)]\n",
    "            for (test_phrase, val) in test_phrases.iteritems():\n",
    "                if (test_phrase not in long_phrases) and not any(test_phrase in long_phrase for long_phrase in long_phrases):\n",
    "                    long_phrases[test_phrase] = val\n",
    "            \n",
    "    return long_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_reps =\"\"\"\n",
    "Nota that There once was a thing that and 5 There once was a thing that and 4 There once was a thing that and 3\n",
    "There once was a thing that and 1 There once was a thing that and  6 There once was a thing that and 7\n",
    "there was another that 1 and there was another that 2 and there was another that 3 and there was another that and\n",
    "there was another that and there was another that 5 and there was another that 9 and there was another that\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c74a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner( txt_reps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_all(txt_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31db255",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_all( row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e70f03",
   "metadata": {},
   "source": [
    "## Longest Common Substring\n",
    "\n",
    "Could we use `difflib.SequenceMatcher.find_longest_match()` on first and second half of doc, or various docs samples, to try to find common refrains?\n",
    "\n",
    "Or chunk into paragraphs and compare every paragraph with every other paragraph?\n",
    "\n",
    "Here's how the to call the `SequenceMatcher().find_longest_match()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SequenceMatcher(None, txt_reps.split('\\n')[1], txt_reps.split('\\n')[2]).find_longest_match()\n",
    "m, txt_reps.split('\\n')[1][m.a: m.a + m.size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16859bce",
   "metadata": {},
   "source": [
    "## Doc2Vec Search Engine\n",
    "\n",
    "To explore: a simple `Doc2Vec` powered search engine based on https://www.kaggle.com/hgilles06/a-doc2vec-search-engine-cord19-new-version ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19009c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
